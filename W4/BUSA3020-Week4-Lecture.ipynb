{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUSA3020 - Advanced Analytics Techniques {-}\n",
    "\n",
    "### Week 4 Lecture - Classification Algorithms (Part 3) {-}\n",
    "\n",
    "### Unit Convenor & Lecturer {-}\n",
    "[George Milunovich](https://www.georgemilunovich.com)  \n",
    "[george.milunovich@mq.edu.au](mailto:george.milunovich@mq.edu.au)\n",
    "\n",
    "### References {-}\n",
    "1. Python Machine Learning 3rd Edition by Raschka & Mirjalili - Chapter 3 (second half)\n",
    "2. Various open-source material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4 Learning Objectives {-}\n",
    "\n",
    "- **Support Vector Machines (SVMs)**\n",
    "    - Maximum Margin Classification\n",
    "    - Dealing With a Nonlinearly Separable Case Using Slack Variables\n",
    "    - Solving Nonlinear Problems Using Kernel SVM\n",
    "    - Kernel Trick\n",
    "    - Implementations in `scikit-learn` \n",
    "- **Decision Tree Learning**\n",
    "    - Maximizing Information Gain\n",
    "    - Building a Decision Tree\n",
    "- **Random Forests**\n",
    "    - Combining Multiple Decision Trees  \n",
    "- **K-nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Support Vector Machines (SVMs) - Maximum Margin Classification {-}\n",
    "\n",
    "Support Vector Machine can be seen as an extension of the perceptron. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/image1.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "![](images/image1.jpg)\n",
    "\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "A key concept in relation to SVMs is the **margin**.\n",
    "- The margin is related to the distance betwen the decision boundary (separating hyperplane) and the training examples that are closest to this boundary\n",
    "- Such closest examples are called **support vectors**\n",
    "\n",
    "SVM attempts to **maximize the margin** thus obtaining maximal separation between the closest elements of two classes\n",
    "- The reason for preferring decision boundaries with large margins is that models with large margins are less likely to be overfitted\n",
    "    - Large margins lead to better performance on test data (lower generalization error)\n",
    "- SVM is less sensitive to outliers than other classification algorithms because it mostly cares about the points closest to the decision boundary\n",
    "- In contrast the perceptron algorithm attempts to minimise misclassification errors\n",
    "\n",
    "<hr style=\"width:25%;margin-left:0;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How SVM finds the maximum margin {-}\n",
    "\n",
    "First consider the case where we have only two features $x_1$ and $x_2$ and remember the equation of the straight line which we can use to plot the decision boundary in the above figure.\n",
    "\n",
    "$w_1x_1 + w_2x_2 + w_0 = 0 \\Rightarrow x_2 = -\\frac{w_0}{w_2}-\\frac{w_1}{w_2}x_1$\n",
    "\n",
    "Now consider the following two lines where the first line is on the positive (+) class side of the boundary and the second is on the negative (-) class side: \n",
    "\n",
    "$w_1x_1^+ + w_2x_2^+ + w_0 = 1 \\Rightarrow x_2^+ = \\left(\\frac{1}{w_2}-\\frac{w_0}{w_2}\\right)-\\frac{w_1}{w_2}x_1^+$    \n",
    "$w_1x_1^- + w_2x_2^- + w_0 = -1 \\Rightarrow x_2^- = \\left(-\\frac{1}{w_2}-\\frac{w_0}{w_2}\\right)-\\frac{w_1}{w_2}x_1^-$\n",
    "\n",
    "As you can see only the intercepts have changed. \n",
    "- The first equation is above the original line while the second equation is below the original line.\n",
    "- By choosing $w_0, w_1$ and $w_2$ we can change the intercept and slope of the decision boundary, as well as how far from it are the lines above and below (positive and negative hyperplanes). \n",
    "\n",
    "How does SVM chose $w_0, w_1$ and $w_2$?\n",
    "\n",
    "To find the distance between the two hyperplanes we subtract the last equation from the one above it:  \n",
    "$w_1(x_1^+ - x_1^-) + w_2(x_2^+ - x_2^-) = 2$\n",
    "\n",
    "Next we divide the above equation by the length of the $\\mathbf{w}=\\left[\\begin{array}{c} \n",
    "w_1\\\\\n",
    "w_2\\\\\n",
    "\\end{array}\\right]$ which is defined as $||w||=\\sqrt{(w_1^2 + w_2^2)}$ to obtain the **distance** between the two hyperplanes which is the **margin**:\n",
    "\n",
    "$\\frac{w_1(x_1^+ - x_1^-) + w_2(x_2^+ - x_2^-)}{\\sqrt{(w_1^2 + w_2^2)}} = \\frac{2}{\\sqrt{(w_1^2 + w_2^2)}}=\\frac{2}{||w||}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background:yellow'> In order to maximize the margin SVM will do the following:\n",
    "\n",
    "- Choose $w_0, w_1, w_2$ to minimize $\\frac{1}{2}||w||^2$ subject to two **constraints**\n",
    "\n",
    "1. $w_0 + w_1x_1^{(i)}+w_2x_2^{(i)}\\ge 1$ if $y^{(i)}=1$ i.e. all positive-class examples fall above the positive hyperplane\n",
    "2. $w_0 + w_1x_1^{(i)}+w_2x_2^{(i)}\\le -1$ if $y^{(i)}=-1$ i.e. all negative-class examples fall below the negative hyperplane\n",
    "\n",
    "</span>\n",
    "\n",
    "This is typically done using quadratic programming and we will not get into mathematical details here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "### Dealing with a nonlinearly separable case using slack variables {-} \n",
    "\n",
    "When dealing with nonlinearly separable data the above constraints will not be satisfied in the presence of misclassification\n",
    "- SVM will not be able to maximize the margin\n",
    "- Introduce **slack variables** $\\xi>0$  -> this is called **soft-margin clasification**\n",
    "- Slack variables will allow the algorithm to converge (find optimal $w$'s) even when dealing with nonlinearly separable data and have classification errors\n",
    "\n",
    "<span style='background:yellow'> SVM will now solve do following problem:\n",
    "\n",
    "- Minimize $\\frac{1}{2}||w||^2 + C\\left(\\sum_i\\xi^{(i)}\\right)$ subject to the following constraints\n",
    "    \n",
    "1. $w_0 + w_1x_1^{(i)}+w_2x_2^{(i)}\\ge 1-\\xi^{(i)}$ if $y^{(i)}=1$\n",
    "2. $w_0 + w_1x_1^{(i)}+w_2x_2^{(i)}\\le -1+\\xi^{(i)}$ if $y^{(i)}=-1$  \n",
    "\n",
    "<br>\n",
    "    \n",
    "Here $C$ deals with how we treat misclassification and control the width of the margin\n",
    "- Large $C$ -> Large error penalties for making misclassification errors\n",
    "    - Decrease Bias\n",
    "    - Increase Variance\n",
    "- Small $C$ -> Small error penalties (less strict about making misclassification errors)\n",
    "    - Increase Bias\n",
    "    - Decrease Variance\n",
    "    \n",
    "\n",
    "<img src=\"images/image2.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "    \n",
    "![](images/image2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "## `scikit-learn` SVM implementation {-} \n",
    "\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(1)    # set random seed to be able to generate SAME random data every time\n",
    "\n",
    "X_xor = np.random.randn(200, 2)\n",
    "\n",
    "y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)\n",
    "\n",
    "# print(np.hstack((X_xor[:10], y_xor[:10].reshape(-1,1))))\n",
    "\n",
    "y_xor = np.where(y_xor, 1, -1)\n",
    "print(np.hstack((X_xor[:10], y_xor[:10].reshape(-1,1))))\n",
    "\n",
    "\n",
    "plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], c='b', marker='x', label='1') # columns 0 and 1\n",
    "plt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1], c='r', marker='s', label='-1')\n",
    "\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "To employ SVM we will use `scikit-learn` implementation \n",
    "- [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "\n",
    "- Lets also put our `plot_decision_regions` function from Week 3 into `plot_decision_regions.py` file\n",
    "\n",
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "```\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import plot_decision_regions as pdr\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=1)\n",
    "svm.fit(X_xor, y_xor)\n",
    "\n",
    "pdr.plot_decision_regions(X_xor, y_xor, classifier=svm)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Accuracy = {svm.score(X_xor, y_xor):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "### Linearly Inseparable Data and Kernel Methods {-} \n",
    "\n",
    "- As discussed before linearly inseparable data is data which is not perfectly separable using a line, a plane or a hyperplane\n",
    "\n",
    "<img src=\"images/pic3.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "![](images/pic3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure illustrates linearly inseparable data\n",
    "- None of the classification algorithms we have covered so far would be able to classify linearly inseperable data accurately\n",
    "\n",
    "**Kernel Methods**\n",
    "- Deal with linearly inseparable data\n",
    "- Employ a nonlinear function $\\phi$\n",
    "- Create a nonlinear function of the orginal features using $\\phi$ creating another dimension in the feature space\n",
    "- Data becomes linearly separable in the higher dimensional space\n",
    "- This allows us to separate the two classes via a linear hyperplane that becomes a nonlinear decision boundary when we project is back to the original feature space\n",
    "\n",
    "\n",
    "Consider the following case\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/image3.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "![](images/image3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the following: \n",
    "- Orginal features are $x_1$ and $x_2$ \n",
    "- We can transform the original two-dimensional feature space into a three-dimensional feature space as follows \n",
    "- This can be done as follows $\\phi(x_1, x_2)=(z_1, z_2, z_3)=(x_1, x_2, x_1^2 + x_2^2)$\n",
    "- the transformation we used here employs polynomial kernel, but there are many other functions which are used\n",
    "- a visualisation of this example is here [https://youtu.be/OdlNM96sHio](https://youtu.be/OdlNM96sHio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "### The Kernel Trick {-}\n",
    "\n",
    "The method we used above can be described as follows\n",
    "- Transform the orignial data using the function $\\phi$\n",
    "- This creates a higher dimensional feature space\n",
    "- Train a linear SVM to clasify the data in the new feature space\n",
    "- When we wish to classify test data transform it using $\\phi$ as well\n",
    "- Apply trained SVM to tranformed test data\n",
    "\n",
    "This approach is complicated and can be computationally expensive, especially if we have hundreds of features\n",
    "\n",
    "- Most machine learning methods manipulate data using the dot (inner) product on feature vectors $x^{(i)^T}x^{(j)}$\n",
    "    - remember that $x^{(i)^T}x^{(j)}=x_1^{(i)}x_1^{(j)}+x_2^{(i)}x_2^{(j)}+\\dots+x_m^{(i)}x_m^{(j)}=\\sum_{\\ell=1}^mx_\\ell^{(i)}x_\\ell^{(j)}$ \n",
    "- When tranforming data using $\\phi$ instead of $x^{(i)^T}x^{(i)}$ we have $\\phi(x^{(i)})^T\\phi(x^{(j)})$\n",
    "    - computing this part is computatinally expensive\n",
    "    \n",
    "Kernel Trick: instead of first choosing $\\phi$ and computing $\\phi(x^{(i)})^T\\phi(x^{(j)})$ we define a **kernel function**\n",
    "\n",
    "$\\kappa\\left(x^{(i)}, x^{(j)}\\right)=\\phi(x^{(i)})^T\\phi(x^{(j)})$ directly\n",
    "\n",
    "and use it instead of $\\phi(x^{(i)})^T\\phi(x^{(j)})$. The whole idea is that $\\kappa\\left(x^{(i)}, x^{(j)}\\right)$ is easier to compute.\n",
    "\n",
    "There are a number of kernel functions that we use in practice. For instance we have\n",
    "\n",
    "**Radial basis function (RBF) kernel** also known as the **Gaussian kernel**\n",
    "\n",
    "$\\kappa\\left(x^{(i)}, x^{(j)}\\right)=\\text{exp}\\left(-\\frac{||x^{(i)}-x^{(j)}||^2}{2\\sigma^2}\\right)=\\text{exp}\\left(-\\gamma||x^{(i)}-x^{(j)}||^2\\right)$\n",
    "\n",
    "where $\\gamma=\\frac{1}{2\\sigma^2}$ is a hyperparameter\n",
    "- <span style='background:yellow'> increasing $\\gamma$ will lead to a tighter and less smooth decision boundary\n",
    "    \n",
    "- Besides the RBF kernel there are many other kernel functions, some of which are implemented in scikit-learn\n",
    "    - [https://scikit-learn.org/stable/modules/metrics.html](https://scikit-learn.org/stable/modules/metrics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Implementing a Nonlinear Kernel with SVM {-}\n",
    "\n",
    "- In our previous application of SVM above we used a `linear` kernel\n",
    "    - Next we will use `rbf` kernel \n",
    "- Consider two values for $\\gamma$ between 0.10 and 100\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "- Later in the course we will learn how to find optimal values for hyperparameters such as $\\gamma$, kernel and $C$\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "svm2 = SVC(kernel='rbf', gamma = 0.10, C=10.0, random_state=1)\n",
    "svm2.fit(X_xor, y_xor)\n",
    "\n",
    "pdr.plot_decision_regions(X_xor, y_xor, classifier=svm2)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Accuracy = {svm2.score(X_xor, y_xor):.3f}')\n",
    "```\n",
    "\n",
    "```# ----------------------------------------```\n",
    "\n",
    "```\n",
    "svm3 = SVC(kernel='rbf', gamma = 100, C=10.0, random_state=1)\n",
    "svm3.fit(X_xor, y_xor)\n",
    "\n",
    "pdr.plot_decision_regions(X_xor, y_xor, classifier=svm3)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Accuracy = {svm3.score(X_xor, y_xor):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Decision Tree Learning {-}\n",
    "- Decision trees can build complex decision boundaries by dividing the feature space into rectangles\n",
    "- Decision tree model learns a series of questions regarding the features in the training set to infer the class labels of the examples\n",
    "    - Here we have to be careful\n",
    "     - The deeper a decision tree is, the more complex the decision boundary becomes, which can easily result in overfitting\n",
    "- High interpretability of results\n",
    "- Decision Trees (and Random Forests below) do not require feature scaling to be performed as they are not sensitive to the the variance in the data\n",
    "- Consider deciding what to do on a particular day with the following class labels `{stay in, go to beach, go running, go to movies}`\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/image5.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to build a decision tree {-}\n",
    "\n",
    "- Start at the tree root and split the data on the feature which results in the largest **information gain - IG**\n",
    "- Repeat the splitting procedure at each child node until all training examples at each node belong to the same class (leaves are **pure**)\n",
    "    - This can result in very deep trees with many nodes which are overfitted\n",
    "- To avoid overfitting we set the maximal depth for the tree (we **prune** the tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "### Maximizing Information Gain (IG) {-}\n",
    "\n",
    "While each node can be split in many child nodes, most libraries (including scikit-learn) implements **binary decision trees** (each node has two child nodes).\n",
    "\n",
    "We split the nodes at most informative features by optimizing an objective function\n",
    "\n",
    "$IG(D_p, f)=I(D_p)-\\frac{N_\\text{left}}{N_\\text{p}}I(D_\\text{left}) - \\frac{N_\\text{right}}{N_\\text{p}}I(D_\\text{right})$\n",
    "\n",
    "- $f$ - feature to perform the split, e.g. age or education level\n",
    "- $D_p$ - dataset of the parent node\n",
    "- $N_p$ - number of training examples at the parent node\n",
    "- $D_j$ - dataset of the jth child node\n",
    "- $N_j$ - number of training examples in the jth child node\n",
    "- $I$ - Impurity measure\n",
    "\n",
    "Information Gain (IG) is the difference between the impurity of the parent node and the sum of the child node impurities\n",
    "- The node impurity is a measure of the homogeneity (sameness) of the labels at the node\n",
    "- The lower the impurities of the child nodes the larger the information gain\n",
    "\n",
    "\n",
    "Let $p(i|t)$ be the proportion of the examples that belong to class $i$ for a node $t$\n",
    "- E.g. $p(i=1|t)=0.5$ and $p(i=0|t)=0.5$\n",
    "- Let there be $c$ number of classes in total, e.g. $c=2$ as above\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We typically use 3 types of impurity measure:\n",
    "1. **Entropy** \n",
    "- $I_H=-\\sum_{i=1}^cp(i|t)\\text{log}_2p(i|t)$\n",
    "- Maximal if classes are perfectly mixed\n",
    "- E.g. if $p(i=1|t)=1$ in which case $p(i=0|t)=0$ $\\Rightarrow I_H=-\\left(1(0) + 0(-\\infty)\\right)=0$  \n",
    "- E.g. if $p(i=1|t)=0.5$ in which case $p(i=0|t)=0.5$ $\\Rightarrow I_H=-\\left(0.5\\text{log}_2(0.5) + 0.5\\text{log}_2(0.5)\\right)=1$\n",
    "\n",
    "2. **Gini Impurity** \n",
    "- $I_G=\\sum_{i=1}^cp(i|t)(1-p(i|t))$\n",
    "- Maximal if classes are perfectly mixed\n",
    "- E.g. if $p(i=1|t)=1$ in which case $p(i=0|t)=0$ $\\Rightarrow I_G=\\left(1(0) + 0(1)\\right)=0$  \n",
    "- E.g. if $p(i=1|t)=0.5$ in which case $p(i=0|t)=0.5$ $\\Rightarrow I_G=\\left(0.5(0.5) + 0.5(0.5)\\right)=0.5$\n",
    "\n",
    "3. **Classification Error** ($I_E$)\n",
    "- $I_E = 1- \\text{max}[p(i|t)]$\n",
    "- Maximal if classes are perfectly mixed\n",
    "- Less sensitive to changes in class probabilities of the nodes\n",
    "- E.g. $p(i=1|t)=1$ in which case $p(i=0|t)=0 \\Rightarrow I_E=1 -1 = 0$  \n",
    "- E.g. $p(i=1|t)=0.5$ in which case $p(i=0|t)=0.5 \\Rightarrow I_E=1 -0.5 = 0.5$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/image10.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "![](images/image10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "Lets consider splitting a parent node which has (40, 40) examples, i.e 40 examples from class 1 and 40 examples from class 2, in two different ways\n",
    "- A: left node: (30, 10), right node: (10, 30)\n",
    "- B: left node: (20, 40), right node: (20, 0)\n",
    "\n",
    "<img src=\"images/image6.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "![](images/image6.jpg)\n",
    "\n",
    "\n",
    "Now we compare the two splits A & B based on the three impurity measure\n",
    "    - Note that B split is purer\n",
    "\n",
    "- Classification Error\n",
    "    - IG = 0.25 under both scenarios\n",
    "    - Make sure you can do these computations\n",
    "\n",
    "\n",
    "<img src=\"images/image7.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "- Gini Impurity\n",
    "    - Gini impurity favours B split (IG = 0.16) over A split (IG = 0.125)\n",
    "    - Make sure you can do these computations\n",
    "    \n",
    "<img src=\"images/image8.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "- Entropy\n",
    "    - Entropy favours B split (IG = 0.31) over A split (IG = 0.19)\n",
    "    - Make sure you can do these computations\n",
    "\n",
    "<img src=\"images/image9.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "![](images/image9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "### Building a decision tree with `scikit-learn` {-} \n",
    "\n",
    "- `scikit-learn` provides [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- As we can see there are a number of parameters to set such as `criterion` and `max_depth`\n",
    "\n",
    "Lets apply a decision tree to our linearly inseparable data generated above\n",
    "- set `max_depth = 4` & `criterion='gini'`\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier(criterion='gini', max_depth = 4, random_state=1)\n",
    "tree_model.fit(X_xor, y_xor)\n",
    "\n",
    "\n",
    "\n",
    "pdr.plot_decision_regions(X_xor, y_xor, classifier=tree_model)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Accuracy = {tree_model.score(X_xor, y_xor):.3f}')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the decision tree model\n",
    "\n",
    "```\n",
    "from sklearn import tree\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "tree.plot_tree(tree_model)\n",
    "plt.savefig('tree1.png')\n",
    "\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "Alternatively we can use `Graphviz` program to produce a nicer image\n",
    "\n",
    "We will need to install three new packages using the following commands\n",
    "\n",
    "`conda install pydotplus`  \n",
    "`conda install graphviz`  \n",
    "`conda install pyparsing`  \n",
    "\n",
    "Restart Anaconda Navigator (do not just close browser window).  \n",
    "We should now be able to use `graphviz` to plot our fitted tree model\n",
    "\n",
    "```\n",
    "from pydotplus import graph_from_dot_data\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image  \n",
    "\n",
    "dot_data = export_graphviz(tree_model, filled=True, rounded=True, class_names=['-1', '+1'], feature_names=['x1', 'x2'], out_file=None)\n",
    "graph = graph_from_dot_data(dot_data)\n",
    "graph.write_png('tree.png')\n",
    "\n",
    "\n",
    "Image(graph.create_png())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Forests {-}\n",
    "\n",
    "**Random Forests** are **ensembles** of decision trees\n",
    "- Average multiple decision tree which may individually suffer from high variance (overfitting)\n",
    "- The process of averaging may reduce the amount of overfitting and result in a model which generalizes better to unseen data\n",
    "- More difficult to interpret\n",
    "\n",
    "**Random Forest Algorithm**\n",
    "1. Draw a random **bootstrap** sample of size $n$ from the training dataset (with replacement)\n",
    "2. Grow a decision tree from the bootstrap sample:\n",
    "    - Randomly select $d$ features without replacement (different from just training one tree)\n",
    "    - Build a tree using these $d$ features\n",
    "3. Repeat 1.-2. $k$ times\n",
    "4. Aggregate the prediction by each tree to assign the class label by **majority vote**\n",
    "    - Every tree makes a prediction (votes) for each test example and the final output prediction is the one that receives more than half of the votes. If none of the predictions get more than half of the votes, we may say that the ensemble method could not make a stable prediction for this instance. \n",
    "\n",
    "**Hyperparameters**\n",
    "- number of trees ($k$) - typically the larger the $k$ the better the perfmance but more computationally expensive\n",
    "- bootstrap size ($n$) - smaller $n$ -> increase randomness of random forest -> can reduce overfitting but also increase bias\n",
    "    - `scikit-learn` sets $n=$sample size of training set (but with replacement)\n",
    "- number of random features used, `scikit-learn` sets $d=\\sqrt{m}$, where $m$ is the total number of features\n",
    "\n",
    "\n",
    "`scikit-learn` implementation: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fitting Random Forest with `scikit-learn` {-}\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(criterion='gini', n_estimators=25, random_state=1, n_jobs=2)\n",
    "forest.fit(X_xor, y_xor)\n",
    "\n",
    "\n",
    "pdr.plot_decision_regions(X_xor, y_xor, classifier=forest)\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Accuracy = {forest.score(X_xor, y_xor):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## K-nearest Neighbors (KNN) {-}\n",
    "\n",
    "KNN algorithm finds $k$ examples in the training set that are closest to the point we try to classify according to a distance metric chosen.\n",
    "\n",
    "KNN is a **lazy learner** -> it does not learn a decision boundary using some function of data but instead memorizes the training dataset\n",
    "    \n",
    "**KNN Algorithm**\n",
    "1. Choose a number $k$ and a distance metric\n",
    "2. Find the $k$-nearest neighbors of the data example we need to classify\n",
    "3. Assign the class label by majority vote\n",
    "\n",
    "\n",
    "Example: classifying a new datapoint (?) based on 5 nearest neighbors from the training set\n",
    "\n",
    "\n",
    "<img src=\"images/image11.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "![](images/image11.jpg)\n",
    "\n",
    "**Hyperparameters**\n",
    "- $k$ - crucial in finding a good balance between overfitting and underfitting\n",
    "- Distance metric -> different metrics will find different neighbors\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Lets find the distance betwen two vectors $x^T=[x_1 \\quad x_2 \\quad ... \\quad x_n]$ and $z^T=[z_1 \\quad z_2 \\quad ... \\quad z_n]$\n",
    "- Minkowski distance (metric) is equal to $d(x,z)=\\sqrt[p]{\\sum_{i=1}^n|x_i - z_i|^p}$  \n",
    "- When $p=2$ we have $d(x,z)=||x-z||=\\sqrt{\\sum_{i=1}^n(x_i - z_i)^2}$ \n",
    "    - Setting $z = 0$ (vector zero) will give us the norm (vector length) of $x$ from before\n",
    "\n",
    "\n",
    "KNN advantage: immediately adapts as we collect new data  \n",
    "KNN disadvantage: computationally complexity grows as we add more examples to our training set\n",
    "\n",
    "`scikit-learn` [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "\n",
    "- Try varying `n_neighbors` over across the values {50, 20, 2}\n",
    "\n",
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "\n",
    "knn.fit(X_xor, y_xor)\n",
    "\n",
    "\n",
    "pdr.plot_decision_regions(X_xor, y_xor, classifier=knn)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Accuracy = {knn.score(X_xor, y_xor):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
