{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Week 7 Lecture & Computer Lab - Best Practices for Model Evaluation and Hyperparameter Tuning {-}\n",
    "\n",
    "\n",
    "### Unit Convenor & Lecturer {-}\n",
    "[George Milunovich](https://www.georgemilunovich.com)  \n",
    "[george.milunovich@mq.edu.au](mailto:george.milunovich@mq.edu.au)\n",
    "\n",
    "### References {-}\n",
    "\n",
    "1. Python Machine Learning 3rd Edition by Raschka & Mirjalili - Chapter 6\n",
    "2. Various open-source material\n",
    "\n",
    "### Week 7 Learning Objectives {-}\n",
    "\n",
    "1. Streamlining Workflows with Pipelines\n",
    "2. The Holdout Method and K-Fold Cross-Validation to Assess Model Performance\n",
    "3. Debugging algorithms with learning and validation curves\n",
    "4. Fine-tuning machine learning models via grid search\n",
    "5. Looking at different performance evaluation metrics\n",
    "  - The confusion matrix\n",
    "  - Optimizing the precision and recall of a classification model\n",
    "  - Plotting a receiver operating characteristic\n",
    "  - The scoring metrics for multiclass classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Streamlining Workflows with Pipelines {-}\n",
    "\n",
    "- `Pipeline` class in scikit-learn is a wrapper tool\n",
    "    - A wrapper is a class whose purpose is to provide a different interface than the thing it wraps \n",
    "        - It usually provides more functionality and interface to it \n",
    "    - It allows us to combine transformers and estimators in one object\n",
    "    - We can make predictions about new data in one step\n",
    "    - E.g. we can standarize features, extract principal components and fit a logistic regression all in one step\n",
    "\n",
    "\n",
    "- `make_pipeline` function takes an arbitrary number of scikit-learn transformers (objects which support `fit` and `transform` methods) followed by an estimator that implements `fit` and `predict` methods\n",
    "\n",
    "\n",
    "- `fit` method of `Pipeline` will pass the data down a series of transformers via `fit` and `transform` calls, until it reaches the estimator object\n",
    "    - The estimator will then be fitted to the transformed training data\n",
    "\n",
    "\n",
    "- `predict` method of `Pipeline` will pass the data through the intermediate steps via `transform` calls\n",
    "    - In the final step the estimator wil return a prediction on the transformed data\n",
    "    \n",
    "<img src=\"images/06_01.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<!-- Image(filename='images/06_01.png', width=500)  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "### Loading the Breast Cancer Wisconsin dataset {-}\n",
    "\n",
    "Breast Cancer Wisconsin dataset\n",
    "- 569 examples of malignant or benign tumor cells\n",
    "- 1st column - unique ID number of patient\n",
    "- 2nd column - diagnosis: M = Malignant, B = Benign\n",
    "- Columns 2 - 31 represent 30 features computed from digitized images of the cell nuclei used to build a model to predict whether a tumor is benign or malignant\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "# df.to_csv('data/wdbc.data', index=False, header = None)\n",
    "\n",
    "# if the Breast Cancer dataset is temporarily unavailable from the\n",
    "# UCI machine learning repository, un-comment the following line\n",
    "# of code to load the dataset from a local path:\n",
    "\n",
    "df = pd.read_csv('data/wdbc.data', header = None)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(10)\n",
    "\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  Example: Combining transformers and estimators in a pipeline   \n",
    "\n",
    "1. Create $X$ and $y$ variables \n",
    "2. Encode target with `LabelEncoder`\n",
    "3. Split dataset into train & test (20%) datasets stratifying by $y$   \n",
    "4. Use `make_pipeline` to\n",
    "    - Standardize data (`StandardScaler`),\n",
    "    - Extract two principal components (`PCA`)\n",
    "    - Logistic Regression (`LogisticRegression`)\n",
    "    \n",
    "5. Fit to train dataset and compute accuracy on training data\n",
    "6. Make predictions using test data and compute accuracy on test data \n",
    "7. Repeat steps 4 - 6 by doing step-by-step of\n",
    "    - Scaling data\n",
    "    - Extracting principal components\n",
    "    - Fitting Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create $X$ and $y$ variables \n",
    "\n",
    "```\n",
    "print(df.shape)\n",
    "\n",
    "y = df.loc[:, 1].values\n",
    "X = df.loc[:, 2:].values\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "y[:25]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encode target with `LabelEncoder`\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(le.classes_)\n",
    "y  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split dataset into train & test (20%) datasets stratifying by $y$   \n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use `make_pipeline` to\n",
    "    - Standardize data (`StandardScaler`),\n",
    "    - Extract two principal components (`PCA`)\n",
    "    - Logistic Regression (`LogisticRegression`)\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=2),\n",
    "                        LogisticRegression(random_state=1, solver='lbfgs'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fit to train dataset and compute accuracy on training data\n",
    "\n",
    "```\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "print(f'Test Accuracy: {pipe_lr.score(X_train, y_train):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make predictions using test data and compute accuracy on test data \n",
    "\n",
    "\n",
    "```\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "print(f'Test Accuracy: {pipe_lr.score(X_test, y_test):.3f}')\n",
    "\n",
    "# y_pred = pipe_lr.predict(X_test[0:1, :])\n",
    "# y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Repeat steps 4 - 6 by doing step-by-step of\n",
    "    - Scaling data\n",
    "    - Extracting principal components\n",
    "    - Fitting Logistic Regression\n",
    "    \n",
    "    \n",
    "```\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X_train_scaled_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled_pca = pca.transform(X_test_scaled)    \n",
    "\n",
    "lr = LogisticRegression(random_state=1, solver='lbfgs')\n",
    "lr.fit(X_train_scaled_pca, y_train)\n",
    "print(f'Train Accuracy: {lr.score(X_train_scaled_pca, y_train):.3f}')\n",
    "\n",
    "print(f'Test Accuracy: {lr.score(X_test_scaled_pca, y_test):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# The Holdout Method and K-Fold Cross Validation to Assess Model Performance {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choosing the best possible model for forecasting relies on \n",
    "    - Training the model on the training dataset\n",
    "    - Testing how well the model generalises on test data\n",
    "    - However... \n",
    "        - Also need to **tune hyperparameters** and compare different settings to further improve performance on unseen data\n",
    "            - A hyperparameter is a parameter whose value is used to control the learning process \n",
    "            - By contrast, the values of other parameters (typically called weights) are derived via training \n",
    "        - **Model selection**: Selection of optimal values of **tuning parameters** (hyperparameters)\n",
    "        - Problem: If we reuse the same test dataset over and over again during **model selection**, it will become part of our training data and thus the model will be more likely to overfit\n",
    "    \n",
    "    \n",
    "      \n",
    "- Need to be able to estimate how well the model can generalize (perform on unseen data) even when tuning hyperparameters\n",
    "    - Holdout Cross-Validation\n",
    "    - K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## The Holdout Method {-}\n",
    "\n",
    "- Split data into three parts:\n",
    "1. Training dataset - fit different models and same models with different hyperparameter values\n",
    "2. Validation dataset - **Model Selection**: repeatedly evaluate the performance of the model after training using different hyperparameter values\n",
    "     - Choose best hyperparameter value\n",
    "3. Test dataset - unseen data used to estimate final model's ability to generalise to new data\n",
    "\n",
    "  \n",
    "- **Disadvantage**: \n",
    "    - Performance estimate is sensitive to how we partition the training dataset into the training and validation subsets\n",
    "\n",
    "<img src=\"images/06_02.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## K-fold Cross-Validation {-}\n",
    "\n",
    "- Split the **training set** into $k$ folds without replacement\n",
    "    - $k-1$ folds are used for model training\n",
    "    - 1 fold is used for performance evaluation\n",
    "- Repeat this procedure $k$ times until all $k$ folds are used for evaluation (and training)\n",
    "    - Compute average performance across all $k$ folds to obtain a performance estimate that is less sensitive to sub-partitioning of the training data \n",
    "- **Once best hyperparameter values are found** retrain the model on the complete training set (in order to fit on a maximum number of observations)\n",
    "    - Final performance accuracy is computed using the independent test dataset\n",
    "    \n",
    "      \n",
    "Advantages of K-fold Cross-Validation    \n",
    "- Averaging model performance (accuracy) across different validation datasets results in a more **precise** measure of performance, i.e. it will have a lower variance\n",
    "- Each example (observation) is used for validation exactly once\n",
    "    \n",
    "   \n",
    "In the image below $E_i$'s represent Estimated Performance meaure (e.g accuracy)\n",
    "\n",
    "<img src=\"images/06_03.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "- Typically we set $k=10$\n",
    "    - If we work with a **small dataset** increase $k$ in order to be able to train the model on more data\n",
    "        - If very small dataset use $k=n$. This is called Leave-one-out cross-validation (LOOCV)\n",
    "    - If we work with a **larger dataset** we can decrease $k$, e.g. $k=5$\n",
    "    - If we have unequal class proportions do **stratified cross-validation**\n",
    "        - Use `from sklearn.model_selection import StratifiedKFold`\n",
    "\n",
    "\n",
    "Previously we computed training set accuracy to be 0.954\n",
    "- Lets re-compute this accuracy using K-Fold Cross-Validation\n",
    "    - Note: in this example we do not tune hyperparameters yet\n",
    "    - We just want to see how to average validation accuracies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# ---------------- Stratified KFold -------------------------\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "# print(type(y_train.values))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
    "\n",
    "\n",
    "# for k, (train, test) in enumerate(kfold):\n",
    "#     print(k, '\\n', test)\n",
    "    # print(k,'\\n', y_train[test]) \n",
    "    # print(k,'\\n', np.bincount(y_train[train]) /len(train) ,'\\n', np.bincount(y_train[test])/len(test))\n",
    "\n",
    "scores = []\n",
    "\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    pipe_lr.fit(X_train[train], y_train[train])   # use pipe_lr to endure that all transformations are done properly on each subsample\n",
    "    score = pipe_lr.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    print(f'Fold: {k+1}, Acc: {score}')\n",
    "\n",
    "print(f'\\nCV accuracy: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "In practice we are likely to use `from sklearn.model_selection import cross_val_score`\n",
    "- `cross_val_score` library implements a **k-fold cross-validation** scorer\n",
    "- Allows us to do **stratified k-fold cross-validation** with less code\n",
    "    - We don't need to loop through folds using `for`\n",
    "- [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_v2 = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=1)\n",
    "\n",
    "# print(scores_v2)\n",
    "\n",
    "print(f'CV accuracy scores\\n {scores_v2.reshape(-1,1)}')\n",
    "print(f'CV accuracy: {np.mean(scores_v2):.3f} +/- {np.std(scores_v2):.3f}')\n",
    "print(f'Correlation between scores and scores2: {np.corrcoef(scores, scores_v2)[0,1]}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Fine-Tuning Hyperparameter via Grid Search {-}\n",
    "\n",
    "There are two types of parameters in Machine Learning\n",
    "- Weights which are learned (estimated) from the training data\n",
    "- Hyperparamters (tuning parameters) which are set by the investigator, e.g. $C$ regularization parameter in Logistic Regression \n",
    "    - Different values of hyperparameters result in different forecast accuracy\n",
    "    - How can we choose best hyperparameter values?\n",
    "        - Try a lot of different values \n",
    "        - **Grid Search** - a popular hyperparameter optimization technique\n",
    "    \n",
    "      \n",
    "<hr style=\"width:35%;margin-left:0;\">    \n",
    "\n",
    "## Tuning hyperparameters via grid search {-}\n",
    "- Brute-force exhaustive search method\n",
    "    - Systematically list a lot of (sometimes all) possible values for the solution and check which value provides best solution  \n",
    "- Specify a list of values for different hyperparameters\n",
    "- Algorithm evaluates the model performance for each combination to obtain the **optimal combination** of hyperparameter values from the specified set\n",
    "- `from sklearn.model_selection import GridSearchCV`\n",
    "- It is also possible to use RandomizedSearchCV to randomly sample parameter combinations via randomized search\n",
    "    - It has been shown that if we sample 60 parameter combinations we have 95% chance of obtaining solutions within 5% of the optimal performance \n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "\n",
    "<span style='background:orange'>   **Example**\n",
    "- Use `GridSearchCV` with a Support Vector Machine classifier\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "- Optimize the following hyperparameters\n",
    "    - Regularization parameters $C$ - inverse of regularization strength\n",
    "    - Kernel: linear and rbf\n",
    "    - $\\gamma$ parameter for rbf kernel\n",
    "- Print parameters and accuracy of the best model\n",
    "    - Export best model and print accuracy on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. Create a `pipeline` containing\n",
    "- `StandardScaler`\n",
    "- `SVC`\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe_svc = make_pipeline(StandardScaler(),\n",
    "                         SVC(random_state=1, probability=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2. Define a parameter range & parameter grid\n",
    "\n",
    "```\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]  # range of values for C and gamma\n",
    "\n",
    "\n",
    "param_grid = [{'svc__C': param_range,   # range of values for all parameters\n",
    "               'svc__kernel': ['linear']},\n",
    "              {'svc__C': param_range, \n",
    "               'svc__gamma': param_range, \n",
    "               'svc__kernel': ['rbf']}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3. \n",
    "- Initialize `GridSearchCV`\n",
    "- Fit to data\n",
    "- Print accuracy and optimized hyperparameters of best model\n",
    "\n",
    "```\n",
    "gs = GridSearchCV(estimator=pipe_svc,      # initialise gs object\n",
    "                  param_grid=param_grid2, \n",
    "                  scoring='accuracy', \n",
    "                  refit=True,              # this will refit the best estimator to the whole dataset automatically\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "\n",
    "gs = gs.fit(X_train, y_train)            # fit gs\n",
    "\n",
    "print(gs.best_score_)\n",
    "\n",
    "print(gs.best_params_)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4. Export best model \n",
    "- Compute accuracy on test data\n",
    "- Print predicted labels for test data \n",
    "\n",
    "```\n",
    "best_classifier = gs.best_estimator_                # copy best estimator\n",
    "\n",
    "print(best_classifier)\n",
    "print(f'Test accuracy: {best_classifier.score(X_test, y_test):.3f}')\n",
    "print(best_classifier.predict(X_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Debugging Algorithms with Learning and Validation Curves {-}\n",
    "\n",
    "Two diagnostic tools to help improve the performance of a learning algorithm\n",
    "- Learning Curves - performance as a function of sample size\n",
    "- Validation Curves - performance as a function of model hyperparameters\n",
    "\n",
    "## Diagnosing Bias and Variance Problems with Learning Curves {-}\n",
    "\n",
    "If a model is too complex - has too many parameters - it is likely to overfit\n",
    "- By increasing the sample size we can reduce the extent of overfitting\n",
    "    - In practice it may be very costly or impossible to collect more data\n",
    "- Plot model **training** and **validation** accuracy as functions of training dataset size\n",
    "    - Detect whether the model suffers from high variance or high bias\n",
    "    - Find out if more data can reduce these problems\n",
    "   \n",
    "<img src=\"images/06_04.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model in upper-left corner\n",
    "- Low training and cross-validation accuracy -> model underfits -> high bias\n",
    "- Possible solution - increase the number of parameters\n",
    "    - Collect or construct additional features\n",
    "    - Decrease the degree of regularization\n",
    "    \n",
    "  \n",
    "Model in upper-right corner\n",
    "- Large variability between training and cross-validation accuracy -> model overfits -> high variance\n",
    "    - Collect more training data\n",
    "    - Reduce the complexity of the model (number of parameters/features) - e.g. via Feature Extraction/Selection\n",
    "    - Increase the degree of regularization\n",
    "    \n",
    "In scikit-learn use `from sklearn.model_selection import learning_curve`\n",
    "- By default uses **stratified k-fold cross-validation**\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html)\n",
    "- Lets implement this on our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "# pipe_lr = make_pipeline(StandardScaler(), \n",
    "#                         LogisticRegression(penalty='l2', \n",
    "#                                            random_state=1, \n",
    "#                                            solver='lbfgs', \n",
    "#                                            max_iter=10000)) # increase number of iterations for optimization algorithm in order to avoid potential problems with small datasets\n",
    "\n",
    "\n",
    "# train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, \n",
    "#                                X=X_train,\n",
    "#                                y=y_train,\n",
    "#                                train_sizes=np.linspace(0.1, 1.0, 11), # train_sizes are set up here, split interval [0,1] into 11 evenly spaced pieces\n",
    "#                                cv=10,\n",
    "#                                n_jobs=1)\n",
    "\n",
    "\n",
    "# # print('train sizes', train_sizes)\n",
    "# # print('train scores', train_scores.shape, train_scores)\n",
    "# # print('test scores', test_scores.shape, test_scores)\n",
    "\n",
    "# train_mean = np.mean(train_scores, axis=1)\n",
    "# train_std = np.std(train_scores, axis=1)\n",
    "# test_mean = np.mean(test_scores, axis=1)\n",
    "# test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# plt.plot(train_sizes, train_mean,\n",
    "#          color='blue', marker='o',\n",
    "#          markersize=5, label='Training accuracy')\n",
    "\n",
    "# plt.fill_between(train_sizes,      # plot mean +- one standard deviation          \n",
    "#                  train_mean + train_std,\n",
    "#                  train_mean - train_std,\n",
    "#                  alpha=0.15, color='blue')\n",
    "\n",
    "# plt.plot(train_sizes, test_mean,\n",
    "#          color='green', linestyle='--',\n",
    "#          marker='s', markersize=5,\n",
    "#          label='Validation accuracy')\n",
    "\n",
    "# plt.fill_between(train_sizes,\n",
    "#                  test_mean + test_std,\n",
    "#                  test_mean - test_std,\n",
    "#                  alpha=0.15, color='green')\n",
    "\n",
    "# plt.grid()\n",
    "# plt.xlabel('Number of training examples')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.ylim([0.8, 1.03])\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig('images/06_05.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Conclusion:**\n",
    "- The model performs quite well for sample sizes of about 250 observations or greater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "## Addressing Over- and Under-Fitting with Validation Curves {-}\n",
    "\n",
    "- Validation Curves - performance as a function of model hyperparameters\n",
    "    - `from sklearn.model_selection import validation_curve`\n",
    "    - `validation_curve` employs **stratified k-fold cross-validation** to estimate the performance of the classifier\n",
    "    - Need to specify the parameter we wish to evaluate in `validation_curve`\n",
    "    - Also need to specify a parameter range that the evaluated parameter will take\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html)\n",
    "    \n",
    "Example\n",
    "- Employ a validation curve on LogisticRegression \n",
    "- Vary $C$ - the inverse of the regularization parameter - over the range `[0.001, 0.01, 0.1, 1.0, 10.0, 100.0]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import validation_curve\n",
    "\n",
    "\n",
    "# param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# train_scores, test_scores = validation_curve(\n",
    "#                 estimator=pipe_lr, \n",
    "#                 X=X_train, \n",
    "#                 y=y_train, \n",
    "#                 param_name='logisticregression__C', \n",
    "#                 param_range=param_range,\n",
    "#                 cv=10)\n",
    "\n",
    "# train_mean = np.mean(train_scores, axis=1)\n",
    "# train_std = np.std(train_scores, axis=1)\n",
    "# test_mean = np.mean(test_scores, axis=1)\n",
    "# test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# plt.plot(param_range, train_mean, \n",
    "#          color='blue', marker='o', \n",
    "#          markersize=5, label='Training accuracy')\n",
    "\n",
    "# plt.fill_between(param_range, train_mean + train_std,\n",
    "#                  train_mean - train_std, alpha=0.15,\n",
    "#                  color='blue')\n",
    "\n",
    "# plt.plot(param_range, test_mean, \n",
    "#          color='green', linestyle='--', \n",
    "#          marker='s', markersize=5, \n",
    "#          label='Validation accuracy')\n",
    "\n",
    "# plt.fill_between(param_range, \n",
    "#                  test_mean + test_std,\n",
    "#                  test_mean - test_std, \n",
    "#                  alpha=0.15, color='green')\n",
    "\n",
    "# plt.grid()\n",
    "# plt.xscale('log')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlabel('Parameter C')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.ylim([0.8, 1.0])\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig('images/06_06.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Small $C$ -> regularization is too strong -> both training and validation accuracy is relatively low -> underfitting\n",
    "- $C\\in\\{0.1, 1\\}$ seems to be best in terms of both training and validation accuracy\n",
    "- $C > 1$ -> regularization is too weak -> too many parameters -> training accuracy is good but validation accuracy decreases -> overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Looking at Different Performance Evaluation Metrics {-}\n",
    "\n",
    "- So far we have relied on **prediction accuracy** as a measure of performance\n",
    "- Now we'll look at several other measures of performance\n",
    "\n",
    "### The Confusion Matrix {-}\n",
    "This should be familiar from basic statistics \n",
    "\n",
    "- The null hypothesis $H_0$ is usually something that we assume prior to test, e.g. innocent until proven guilty, healthy until tested positive for a desease, etc\n",
    "    - Negative Result -> Accept H0: e.g. negative COVID test -> accept H0: no covid\n",
    "    - Positive Result -> Reject H0: e.g. positive COVID test -> reject H0: no covid\n",
    "- We can extend this as follows\n",
    "    - False Positive (FP) -> False = Incorrect, Positive = Reject H0 -> Reject H0 when True = Type I Error, e.g. diagnosed with COVID when in reality not sick\n",
    "    - False Negative (FN) -> False = Incorrect, Negative = Accept H0 -> Accept H0 when False = Type II Error, e.g. not diagnosed with COVID when in reality sick\n",
    "\n",
    "\n",
    "- Use `from sklearn.metrics import confusion_matrix`\n",
    "    - Need to specify true classes (target) and predicted classes (prediction of target)\n",
    "\n",
    "<img src=\"images/06_08.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  **Example** \n",
    "   \n",
    "- Retrain `pipe_svc` on train dataset\n",
    "- Produce predictions using `X_test` dataset\n",
    "- Compute and plot confusion matrix\n",
    "- Interpret the results\n",
    "    \n",
    "<hr style=\"width:35%;margin-left:0;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. \n",
    "- Retrain `pipe_svc` on train dataset\n",
    "- Produce predictions using `X_test` dataset\n",
    "\n",
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe_svc.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. \n",
    "\n",
    "- Compute and plot confusion matrix\n",
    "- Interpret the results\n",
    "\n",
    "```\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "# ---------- Plotting \n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "        \n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/06_09.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset there are classes: 0 = Benign tumor and 1 = Malignant tumor  \n",
    "\n",
    "$H_0$: Benign tumor (class 0)  \n",
    "$H_1$: Malignant tumor (class 1)\n",
    "\n",
    "- Correctly Classified: 71 examples of class 0 and 40 examples of class 1\n",
    "- Misclassified: \n",
    "    - False Positive (false rejection of $H_0$) 1 example\n",
    "    - False Negative (false acceptance of $H_0$) 2 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## Optimizing the Precision and Recall of a Classification Model {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction Error: $ERR=\\frac{\\text{Misclassified}}{N}=\\frac{FP+FN}{FP+FN+TP+TN}$\n",
    "  \n",
    "- Accuracy: $ACC = \\frac{\\text{Correctly Classified}}{N}=\\frac{TP+TN}{FP+FN+TP+TN} = 1 - ERR$\n",
    "\n",
    "We select the best forecasting model based on some performance measure, such as accuracy above\n",
    "- However, in some circumstances we may want to choose a model according to some other performance measure, such as the TPR below\n",
    "\n",
    "\n",
    "<img src=\"images/06_08.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "  \n",
    "<hr style=\"width:20%;margin-left:0;\">   \n",
    "\n",
    "**True Positive and False Positive Rates**\n",
    "\n",
    "\n",
    "- True Positive Rate = $TPR = \\frac{TP}{P}=\\frac{TP}{TP+FN}$ \n",
    "    - Probablity of correctly detecting effect\n",
    "    - TPR is known as **power** in statistical testing\n",
    "     \n",
    "       \n",
    "- False Positive Rate = $FPR = \\frac{FP}{N}=\\frac{FP}{FP+TN}$\n",
    "    - Probability of incorrectly detecting effect\n",
    "    - Known as the Probability of Type I Error in statistical testing\n",
    "\n",
    "    \n",
    "- More concerned with TPR - detecting malignant tumors\n",
    "    - Want TPR as high as possible\n",
    "    - Optimizing TPR therefore may be more important than choosing the model with the highest accuracy in this case\n",
    "- It is also important to miminize FPR - not to concern patients unnecessarily\n",
    "\n",
    "<hr style=\"width:20%;margin-left:0;\">   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background:orange'>  \n",
    "Plotting Confusion Matrix v.2\n",
    "    \n",
    "```\n",
    "labels = [1,0]\n",
    "confmat2 = confusion_matrix(y_true=y_test, y_pred=y_pred, labels = labels)\n",
    "\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in confmat2.flatten()]\n",
    "# group_percentages = [\"{0:.2%}\".format(value) for value in confmat2.flatten()/np.sum(confmat2)]\n",
    "group_names = ['True Pos', 'False Neg','False Pos', 'True Neg']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labels3 = [f\"{v2}\\n{v3}\" for v2, v3 in zip(group_names, group_counts)]\n",
    "# labels3 = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "\n",
    "\n",
    "labels3 = np.asarray(labels3).reshape(2,2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(confmat2, annot=labels3, fmt='',xticklabels=labels, yticklabels=labels, cmap='Blues', cbar=False)\n",
    "\n",
    "ax.set_xlabel('Predicted label')    \n",
    "ax.xaxis.set_label_position('top') \n",
    "ax.set_ylabel('True label')\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision (PRE) and Recall (REC)**\n",
    "\n",
    "- $REC=TPR=\\frac{TP}{TP+FN}=0.95$\n",
    "    - Optimizing (choosing) a model based on REC will minimize the chance of not detecting a malignant tumor\n",
    "        - This will however result in some predictions of malignant tumors in healthy patients (FP)\n",
    "- $PRE=\\frac{TP}{TP+FP}=0.98$ \n",
    "    - PRE = 100% will result in having no False Positives\n",
    "        - Likely to miss malignant tumors more frequently (FN)\n",
    "        - But good for spam filtering -> don't want to classify real emails as spam\n",
    "  \n",
    "    \n",
    "- F1 score is a combination of PRE and REC $F1 = 2\\frac{PRE \\times REC}{PRE + REC}$\n",
    "\n",
    "<hr style=\"width:20%;margin-left:0;\"> \n",
    "\n",
    "- In scikit-learn these scoring matrics are already implemented\n",
    "    - `from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score`\n",
    "- If we want to use a scoring metric other than `accuracy` in `GridSearchCV` we can change the `scoring` parameter\n",
    "    - [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_test, y_pred=y_pred):.3f}')\n",
    "print(f'Precision: {precision_score(y_true=y_test, y_pred=y_pred):.3f}')\n",
    "print(f'Recall: {recall_score(y_true=y_test, y_pred=y_pred):.3f}')\n",
    "print(f'F1: {f1_score(y_true=y_test, y_pred=y_pred):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Appendix 1:** The Scoring Metrics for Multiclass Classification {-}\n",
    "\n",
    "- Binary scoring methods can be extended to multiclass problems via one-vs-all (OvA) classification\n",
    "- See textbook for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## **Appendix 2:** Dealing with Class Imbalance {-}\n",
    "\n",
    "- Class Imbalance occures quite often in real world\n",
    "    - Examples from one class or multiple classes are over-represented in a dataset\n",
    "    - Lets say class1 = 90 examples, class2 = 10 examples -> if we just predict class1 for all examples -> 90% accuracy\n",
    "        - If an ML model returns 90% accuracy -> it hasn't really learned anything useful from the features\n",
    "    - In this case accuracy is not the most useful measure of performance\n",
    "        - If want to identify the majority of patients with malignant cancer to recommend an additional screening -> use REC (recall)\n",
    "        - If screening for spam email -> use PRE (precision) (minimise the prob of classifying real emails as spam)\n",
    "    - E.g. fraud detection, loan defaults, etc\n",
    "    \n",
    "\n",
    "**Potential solutions**\n",
    "- Assign a larger penalty to wrong predictions on the minority class\n",
    "    - Set `class_weight='balanced'` parameter\n",
    "    - Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
    "- Upsampling of the minority class\n",
    "    - `resample` in scikit-learn\n",
    "    - Repeatedly draw new samples of the minority class from the dataset with replacement\n",
    "    - Can also Downsample the majority class\n",
    "- Generate new (synthetic) examples of the minority class\n",
    "    - Too technical \n",
    "    - Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "\n",
    "- Lets create an imbalanced dataset -> 357 begnin tumors (class 0) and 40 malignant (class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_imb = np.vstack((X[y == 0], X[y == 1][:40]))\n",
    "# y_imb = np.hstack((y[y == 0], y[y == 1][:40]))\n",
    "\n",
    "# print(y_imb.shape)\n",
    "# print(np.bincount(y_imb))\n",
    "# print(np.bincount(y_imb)/len(y_imb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- So if we predict y = 0 for all samples -> about 90% accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# print('Number of class 1 examples before:', X_imb[y_imb == 1].shape[0])\n",
    "\n",
    "# X_upsampled, y_upsampled = resample(X_imb[y_imb == 1],\n",
    "#                                     y_imb[y_imb == 1],\n",
    "#                                     replace=True,\n",
    "#                                     n_samples=X_imb[y_imb == 0].shape[0],\n",
    "#                                     random_state=123)\n",
    "\n",
    "# print('Number of class 1 examples after:', X_upsampled.shape[0])\n",
    "# pd.DataFrame(X_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_bal = np.vstack((X[y == 0], X_upsampled))\n",
    "# y_bal = np.hstack((y[y == 0], y_upsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_bal.shape)\n",
    "# print(np.bincount(y_bal))\n",
    "# print(np.bincount(y_bal)/len(y_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "### **Appendix 3**: Receiver Operating Characteristic (ROC) {-}\n",
    "\n",
    "Receiver Operating Characteristic (ROC) graphs are useful to select models for classification based on their performance with respect to FPR and TPR  \n",
    "    - [https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)  \n",
    "    - Plot TPR (power) vs FPR (Type I Error)\n",
    "    - Shift the decision threshold of the classifier  \n",
    "    - Diagonal interpreted as random guessing   \n",
    "    - Classification models below the diagonal are worse than random guessing  \n",
    "    - A perfect classifier is in the top-lect corner with a TPR=1 and FPR=0  \n",
    "    - ROC area under the curve (ROC AUC) can be computed to characterize performance\n",
    "        - ROC AUC = 1 -> perfect classifier\n",
    "        - ROC AUC = 0.5 -> random guessing\n",
    "    \n",
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "<span style='background:orange'>  **Example** \n",
    "   \n",
    "- Implement ROC on Breast Cancer Data\n",
    "- Vary X from all features to only 2 features, see how it changes FPR and TPR\n",
    "    \n",
    "<hr style=\"width:35%;margin-left:0;\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from numpy import interp\n",
    "\n",
    "# pipe_lr = make_pipeline(StandardScaler(),\n",
    "#                         PCA(n_components=2),\n",
    "#                         LogisticRegression(penalty='l2', \n",
    "#                                            random_state=1,\n",
    "#                                            solver='lbfgs',\n",
    "#                                            C=100.0))\n",
    "\n",
    "# # X_train2 = X_train[:, [4, 14]]\n",
    "# X_train2 = X_train.copy()\n",
    "\n",
    "\n",
    "# cv = list(StratifiedKFold(n_splits=3).split(X_train, y_train))\n",
    "\n",
    "# fig = plt.figure(figsize=(7, 5))\n",
    "\n",
    "# mean_tpr = 0.0\n",
    "# mean_fpr = np.linspace(0, 1, 100)\n",
    "# all_tpr = []\n",
    "\n",
    "# for i, (train, test) in enumerate(cv):\n",
    "#     probas = pipe_lr.fit(X_train2[train],\n",
    "#                          y_train[train]).predict_proba(X_train2[test])\n",
    "\n",
    "#     fpr, tpr, thresholds = roc_curve(y_train[test],\n",
    "#                                      probas[:, 1],\n",
    "#                                      pos_label=1)\n",
    "#     mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "#     mean_tpr[0] = 0.0\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.plot(fpr,\n",
    "#              tpr,\n",
    "#              label='ROC fold %d (area = %0.2f)'\n",
    "#                    % (i+1, roc_auc))\n",
    "\n",
    "# plt.plot([0, 1],\n",
    "#          [0, 1],\n",
    "#          linestyle='--',\n",
    "#          color=(0.6, 0.6, 0.6),\n",
    "#          label='Random guessing')\n",
    "\n",
    "# mean_tpr /= len(cv)\n",
    "# mean_tpr[-1] = 1.0\n",
    "# mean_auc = auc(mean_fpr, mean_tpr)\n",
    "# plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "#          label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "# plt.plot([0, 0, 1],\n",
    "#          [0, 1, 1],\n",
    "#          linestyle=':',\n",
    "#          color='black',\n",
    "#          label='Perfect performance')\n",
    "\n",
    "# plt.xlim([-0.05, 1.05])\n",
    "# plt.ylim([-0.05, 1.05])\n",
    "# plt.xlabel('False positive rate')\n",
    "# plt.ylabel('True positive rate')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig('images/06_10.png', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
